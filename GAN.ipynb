{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf32647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc, classification_report, ConfusionMatrixDisplay\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2992d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0\n",
    "shuffle_dataset  = True\n",
    "batch_size = 64\n",
    "max_epochs = 100\n",
    "input_size = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd78d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transaction():\n",
    "    df = pd.read_csv(\"transaction_dataset.csv\")\n",
    "\n",
    "    #Rename columns for easier access\n",
    "    df.columns = df.columns.str.strip().str.replace(' ','_').str.lower()\n",
    "\n",
    "    #Remove weird stuff \n",
    "    df.drop(columns=['unnamed:_0'], inplace=True)\n",
    "\n",
    "    #Remove duplicate accounts\n",
    "    df.drop_duplicates(subset=['address'], inplace=True)\n",
    "\n",
    "    #Remove accounts \n",
    "    df.drop(columns=['address'], inplace=True)\n",
    "\n",
    "    #Remove index\n",
    "    df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "    #Remove token names \n",
    "    df.drop(columns=['erc20_most_sent_token_type','erc20_most_rec_token_type'], inplace = True)\n",
    "\n",
    "    #Remove var=0 columns\n",
    "    df.drop(df.var(numeric_only=True)[df.var(numeric_only=True) == 0].index, axis = 1, inplace = True)\n",
    "\n",
    "    #Remove small distribution columns\n",
    "    small_distr_col = []\n",
    "    for col in df.columns[3:] :\n",
    "        if df[col].nunique() < 10:\n",
    "            small_distr_col.append(col)\n",
    "    df.drop(columns=small_distr_col,inplace = True)\n",
    "    \n",
    "    # Remove negative values \n",
    "    df[df<0] = None \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def train_test_split_indices(length, validation_split, shuffle_dataset = True, random_seed = 42):\n",
    "    # Creating data indices for training and validation splits.\n",
    "    indices = np.arange(length)\n",
    "    validation_size = int(validation_split * length)\n",
    "    if shuffle_dataset:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[validation_size:], indices[:validation_size]\n",
    "    return train_indices, val_indices\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, indices, augment = False, \n",
    "                 cols_median = None, cols_means = None, cols_std = None):\n",
    "        super().__init__()\n",
    "        self.augment = augment\n",
    "        \n",
    "        df = df.iloc[indices].copy()\n",
    "        \n",
    "        if(self.augment):\n",
    "            oversample = SMOTE()\n",
    "            df,y = oversample.fit_resample(df.iloc[:,1:],df.values[:,0])\n",
    "        else: \n",
    "            y,df  = df.values[:, 0],df.iloc[:, 1:]\n",
    "        \n",
    "        if any(param is None for param in [cols_median, cols_means, cols_std]):\n",
    "            self.cols_median = df.median(numeric_only=True)\n",
    "            self.cols_means  = df.mean  (numeric_only=True)\n",
    "            self.cols_std    = df.std   (numeric_only=True)\n",
    "        else:\n",
    "            self.cols_median = cols_median\n",
    "            self.cols_means  = cols_means\n",
    "            self.cols_std    = cols_std\n",
    "            \n",
    "        df.fillna(self.cols_median, inplace = True)\n",
    "        df = (df - self.cols_means) / self.cols_std\n",
    "        \n",
    "        self.y = y\n",
    "        self.X = df.values                                    \n",
    "        \n",
    "    def get_cols_stats(self):\n",
    "        return self.cols_median, self.cols_means, self.cols_std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        if self.augment:\n",
    "            pass\n",
    "        \n",
    "        return x, y\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, batch_size, train_indices, val_indices,augment):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.train_indices = train_indices\n",
    "        self.val_indices = val_indices\n",
    "        self.augment = augment\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        self.train_set = MyDataset(self.df, self.train_indices, augment = True)\n",
    "        \n",
    "        train_cols_median, train_cols_means, train_cols_std = self.train_set.get_cols_stats()\n",
    "        \n",
    "        self.val_set = MyDataset(self.df, self.val_indices,\n",
    "                                 augment = False,\n",
    "                                 cols_median = train_cols_median,\n",
    "                                 cols_means  = train_cols_means,\n",
    "                                 cols_std    = train_cols_std)\n",
    "  \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set,\n",
    "                          batch_size = self.batch_size,\n",
    "                          shuffle = True,\n",
    "                          num_workers = 8,\n",
    "                          pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08e3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,file_name,augment=False,path=\"./models\",validation_split=validation_split,\n",
    "          shuffle_dataset=shuffle_dataset,batch_size=batch_size,\n",
    "         max_epochs=max_epochs,input_size=input_size):\n",
    "\n",
    "    df = load_transaction()\n",
    "\n",
    "\n",
    "    train_indices, val_indices = train_test_split_indices(length = len(df),\n",
    "                                                      validation_split = validation_split)        \n",
    "\n",
    "    data = MyDataModule(df,\n",
    "                        train_indices = train_indices,\n",
    "                        val_indices   = val_indices,\n",
    "                        batch_size    = batch_size,\n",
    "                        augment       = augment)\n",
    "    early_stopping = EarlyStopping('val_loss',patience=7)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                            monitor  = \"val_acc\",\n",
    "                            mode     = 'max',\n",
    "                            dirpath  = path,\n",
    "                            filename = file_name)\n",
    "\n",
    "    trainer = pl.Trainer(log_every_n_steps       = 10,\n",
    "                         accelerator             = 'cpu',\n",
    "                         check_val_every_n_epoch = 1,\n",
    "                         enable_checkpointing    = True,\n",
    "                         max_epochs              = max_epochs,\n",
    "                         precision               = 64,\n",
    "                         callbacks               = [checkpoint_callback],\n",
    "                         num_sanity_val_steps    = 0,\n",
    "                         fast_dev_run            = False)\n",
    "\n",
    "    trainer.fit(model, data)\n",
    "    return trainer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4fa85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c17410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32dd9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, out_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x.view(x.size(0), -1))\n",
    "        return x\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x.view(x.size(0), -1)).flatten()\n",
    "\n",
    "        return x\n",
    "class GAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float  = 0.0001,\n",
    "        latent_dim = 50,\n",
    "        input_size = 33\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr         = lr\n",
    "\n",
    "        self.generator = Generator(latent_dim,input_size)\n",
    "        self.discriminator = Discriminator(input_size)\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        addrs, _ = batch\n",
    "\n",
    "        # sample noise\n",
    "        z = torch.randn(addrs.shape[0],self.latent_dim)\n",
    "        z = z.type_as(addrs)\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # ground truth result (ie: all fake)\n",
    "            # put on GPU because we created this tensor inside training_loop\n",
    "            valid = torch.ones(addrs.size(0), 1)\n",
    "            valid = valid.type_as(addrs)\n",
    "\n",
    "            # adversarial loss is binary cross-entropy\n",
    "            g_loss = self.adversarial_loss(self.discriminator(self(z)).view(addrs.size(0), 1), valid)\n",
    "            self.log(\"g_loss\", g_loss, prog_bar=True, on_step = False, on_epoch = True)\n",
    "            return g_loss\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "\n",
    "            # how well can it label as real?\n",
    "            valid = torch.ones(addrs.size(0), 1)\n",
    "            valid = valid.type_as(addrs)\n",
    "\n",
    "            real_loss = self.adversarial_loss(self.discriminator(addrs).view(addrs.size(0), 1), valid)\n",
    "\n",
    "            # how well can it label as fake?\n",
    "            fake = torch.zeros(addrs.size(0), 1)\n",
    "            fake = fake.type_as(addrs)\n",
    "\n",
    "            fake_loss = self.adversarial_loss(self.discriminator(self(z).detach()).view(addrs.size(0), 1), fake)\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            self.log(\"d_loss\", d_loss, prog_bar=True, on_step = False, on_epoch = True)\n",
    "            return d_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr)\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "        \n",
    "        return [opt_g, opt_d], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d250a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 16.9 K\n",
      "1 | discriminator | Discriminator | 14.7 K\n",
      "------------------------------------------------\n",
      "31.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "31.7 K    Total params\n",
      "0.253     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbd6d078a3f41c2bd836eb63a69b696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "gan = train(GAN(),augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63078dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
